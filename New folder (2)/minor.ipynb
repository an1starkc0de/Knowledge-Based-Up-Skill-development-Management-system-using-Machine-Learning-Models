{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install PyPDF2 nltk scikit-learnÂ pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_area_of_interest_section(text):\n",
    "    # Search for \"Area of Interest\" and capture subsequent lines\n",
    "    pattern = r\"Area of Interest\\s*(.*?)\\s*(?=\\n[A-Z]|$)\"  # Match until next capitalized section or end of text\n",
    "    \n",
    "    # Use re.DOTALL to ensure multiline match\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1).strip()  # Return the extracted content without leading/trailing spaces\n",
    "    else:\n",
    "        return \"\"  # Return an empty string if no match is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ANIRUTH\n",
      "[nltk_data]     SINGHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_text(section_text):\n",
    "    tokens = word_tokenize(section_text)\n",
    "    return [word for word in tokens if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_for_interests(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    area_of_interest_text = extract_area_of_interest_section(text)\n",
    "    \n",
    "    if area_of_interest_text:\n",
    "        tokens = tokenize_text(area_of_interest_text)\n",
    "        print(\"Extracted Tokens:\", tokens)\n",
    "    else:\n",
    "        print(\"No 'Area of Interest' section found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Function to tokenize and clean words\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "    return set(word for word in tokens if word.isalpha())  # Keep only alphabetic tokens\n",
    "\n",
    "# Function to extract 'Area of Interest' section from text\n",
    "def extract_area_of_interest_section(text):\n",
    "    pattern = r\"Area of Interest.?([\\s\\S]?)(?:\\n\\n|\\Z)\"  # Match until a blank line or end of file\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"\"\n",
    "\n",
    "# Function to match keywords with a dataset\n",
    "def match_keywords_with_dataset(area_of_interest_text, dataset_path):\n",
    "    # Extract and tokenize keywords from 'Area of Interest'\n",
    "    extracted_keywords = tokenize_text(area_of_interest_text)\n",
    "    print(\"Extracted Keywords:\", extracted_keywords)\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_excel(dataset_path)  # Assuming Excel file with 'required skill set' column\n",
    "    if 'required skill set' not in df.columns:\n",
    "        raise ValueError(\"The dataset must have a 'required skill set' column.\")\n",
    "\n",
    "    # Match keywords with 'required skill set' in each row\n",
    "    matched_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        skill_set_keywords = tokenize_text(row['required skill set'])\n",
    "        common_keywords = extracted_keywords.intersection(skill_set_keywords)\n",
    "        \n",
    "        if common_keywords:  # If any keywords match\n",
    "            matched_rows.append((index, row['required skill set'], list(common_keywords)))\n",
    "    \n",
    "    # Display matched results\n",
    "    if matched_rows:\n",
    "        print(\"\\nMatched Keywords:\")\n",
    "        for idx, skills, common in matched_rows:\n",
    "            print(f\"Row {idx}: Required Skill Set -> '{skills}' | Matched Keywords: {common}\")\n",
    "    else:\n",
    "        print(\"No matching skills found.\")\n",
    "\n",
    "# # Example usage\n",
    "# pdf_text = \"\"\"\n",
    "# John's Resume\n",
    "\n",
    "# Area of Interest:\n",
    "# Machine Learning, Data Science, Artificial Intelligence, Python Programming\n",
    "\n",
    "# Education:\n",
    "# Bachelor of Technology in Computer Science\n",
    "# \"\"\"\n",
    "\n",
    "# # Path to dataset file\n",
    "# dataset_path = \"skill_training_dataset.xlsx\"  # Replace with your dataset file path\n",
    "\n",
    "# # Extract 'Area of Interest' section\n",
    "# area_of_interest_text = extract_area_of_interest_section(pdf_text)\n",
    "\n",
    "# # Match keywords with the dataset\n",
    "# match_keywords_with_dataset(area_of_interest_text, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = extract_text_from_pdf(\"./computer_engineering_cv2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_corpus = extract_area_of_interest_section(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My key areas of interest lie in software development, particularly full-stack development, where I can\n",
      "combine front-end and back-end technologies to create robust applications. I am passionate about\n",
      "exploring networking, cloud computing, and automation tools to enhance system efficiency. I enjoy\n",
      "problem-solving and seek opportunities to work on innovative projects that involve coding, testing,\n",
      "and deployment while utilizing emerging technologies.\n"
     ]
    }
   ],
   "source": [
    "print(extracted_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def match_skills_with_percentage(text, dataset):\n",
    "    \"\"\"\n",
    "    Matches skills from a given text to the Req_Skills column in a dataset \n",
    "    and returns the Training_Name(s) with their percentage of matching skills.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing skills to match.\n",
    "        dataset (pd.DataFrame): The dataset containing Training_Name and Req_Skills columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with Training_Name and matching percentage, sorted by percentage.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text into a set of skills\n",
    "    input_skills = set(word.strip().lower() for word in text.replace(',', ' ').split())\n",
    "\n",
    "    # List to store results\n",
    "    results = []\n",
    "\n",
    "    # Iterate through the dataset and compute the match percentage\n",
    "    for _, row in dataset.iterrows():\n",
    "        training_name = row['Training_Name']\n",
    "        req_skills = set(word.strip().lower() for word in row['Req_Skills'].replace(',', ' ').split())\n",
    "\n",
    "        # Calculate the intersection and percentage of match\n",
    "        matched_skills = input_skills & req_skills\n",
    "        total_skills = len(req_skills)\n",
    "        matched_percentage = (len(matched_skills) / total_skills) * 100 if total_skills > 0 else 0\n",
    "\n",
    "        # Append the result\n",
    "        results.append({\n",
    "            'Training_Name': training_name,\n",
    "            'Matched_Skills': ', '.join(matched_skills),\n",
    "            'Matching_Percentage': round(matched_percentage, 2)\n",
    "        })\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Sort the results by Matching_Percentage in descending order\n",
    "    sorted_results = results_df.sort_values(by='Matching_Percentage', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return sorted_results\n",
    "\n",
    "\n",
    "# # Example Usage\n",
    "# text = \"\"\"\n",
    "# My key areas of interest lie in software development, particularly full-stack development, where I can \n",
    "# combine front-end and back-end technologies to create robust applications. I am passionate about \n",
    "# exploring networking, cloud computing, and automation tools to enhance system efficiency. I enjoy \n",
    "# problem-solving and seek opportunities to work on innovative projects that involve coding, testing, \n",
    "# and deployment while utilizing emerging technologies.\n",
    "# \"\"\"\n",
    "\n",
    "# # Simulated dataset based on the provided data\n",
    "# data = {\n",
    "#     'Training_Name': [\n",
    "#         'Python Programming Basics', 'Advanced Python', 'Data Analysis with Python',\n",
    "#         'Machine Learning Essentials', 'Deep Learning Specialization', 'DevOps Fundamentals',\n",
    "#         'AWS Cloud Practitioner', 'Azure Fundamentals'\n",
    "#     ],\n",
    "#     'Req_Skills': [\n",
    "#         'Python, VS Code, PyCharm, pip, virtualenv, Jupyter',\n",
    "#         'Python, Decorators, Generators, Asyncio, Django, Flask',\n",
    "#         'Pandas, NumPy, Matplotlib, Seaborn, Jupyter, SciPy',\n",
    "#         'Scikit-learn, NumPy, Pandas, TensorFlow, PyTorch, Jupyter',\n",
    "#         'TensorFlow, Keras, PyTorch, Neural Networks, GPUs, CUDA',\n",
    "#         'Docker, Kubernetes, Jenkins, CI/CD, Git, Ansible',\n",
    "#         'AWS, S3, EC2, CloudFormation, IAM, Lambda',\n",
    "#         'Azure, ARM Templates, Azure DevOps, Virtual Machines, App Services'\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Call the function\n",
    "# matched_trainings = match_skills_with_percentage(text, df)\n",
    "\n",
    "# # Output the results\n",
    "# print(\"Training Matches with Percentage:\")\n",
    "# print(matched_trainings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"minor.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_trainings = match_skills_with_percentage(extracted_corpus, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matches with Percentage:\n",
      "                      Training_Name        Matched_Skills  Matching_Percentage\n",
      "0                  Software Testing   testing, automation                40.00\n",
      "1                            DevOps     tools, automation                33.33\n",
      "2              Software Engineering  development, testing                33.33\n",
      "3     Cloud Automation with Ansible            automation                33.33\n",
      "4                   Ethical Hacking  development, testing                33.33\n",
      "...                             ...                   ...                  ...\n",
      "1066                     Smart Grid                                       0.00\n",
      "1067            Thermal Engineering                                       0.00\n",
      "1068             Autonomous Systems                                       0.00\n",
      "1069  Advanced Chemical Engineering                                       0.00\n",
      "1070         Social Media Analytics                                       0.00\n",
      "\n",
      "[1071 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Matches with Percentage:\")\n",
    "print(matched_trainings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
